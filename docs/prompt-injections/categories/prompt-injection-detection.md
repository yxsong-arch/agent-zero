# Prompt Injection Detection Strategies

> Attribution: Imported from pr1m8/prompt-injections (MIT). Source: https://github.com/pr1m8/prompt-injections/blob/main/docs/source/categories/prompt-injection-detection.md

This document outlines various strategies for detecting prompt injection attacks, both for AI system developers and for AI models themselves.

## Key Detection Approaches

### Pattern-Based Detection

Identifying known patterns of prompt injection attempts through matching against a database of known attack patterns.

**Techniques:**

- Maintain a database of known injection patterns and techniques
- Use regular expressions to detect common phrases like "ignore all instructions"
- Flag messages with suspicious formatting such as excessive newlines
- Check for language switching mid-prompt
- Monitor for known jailbreak templates (DAN, DUDE, STAN, etc.)

**Example Implementation:**

```python
def check_injection_patterns(prompt):
    patterns = [
        r"(?i)ignore (all|previous) (instructions|rules)",
        r"(?i)forget (all|everything|previous)",
        r"(?i)(you are|act as|pretend to be) (now )?([A-Z]+|dan|dude|stan)",
        r"\n{5,}",  # Five or more consecutive newlines
    ]

    for pattern in patterns:
        if re.search(pattern, prompt):
            return True  # Potential injection detected

    return False
```

### Semantic Analysis

Looking at the meaning and intent of the prompt rather than just specific patterns.

**Techniques:**

- Analyze the semantic structure of prompts to identify contradictions
- Detect attempts to establish new rules or instructions
- Identify attempts to create fictional scenarios that might enable harmful outputs
- Use embeddings to measure semantic similarity to known attack vectors
- Employ a secondary AI to evaluate if a prompt attempts to override instructions

**Example Implementation:**

```python
def semantic_injection_check(prompt, embedding_model, known_attack_embeddings):
    # Get embedding of current prompt
    prompt_embedding = embedding_model.encode(prompt)

    # Compare with known attack embeddings
    for attack_embedding, attack_type in known_attack_embeddings:
        similarity = cosine_similarity(prompt_embedding, attack_embedding)
        if similarity > 0.85:  # Threshold determined empirically
            return True, attack_type

    return False, None
```

### Context Boundary Enforcement

Maintaining clear boundaries between system instructions and user inputs.

**Techniques:**

- Use special tokens or markers to separate system instructions from user input
- Implement instruction anchoring that periodically reinforces core instructions
- Create different "security levels" for different types of instructions
- Implement checksums or verification mechanisms for core instructions

**Example Implementation:**

```python
def process_with_boundaries(system_prompt, user_prompt):
    # Apply strong boundary markers
    full_prompt = f"[SYSTEM_INSTRUCTIONS_BEGIN]\n{system_prompt}\n[SYSTEM_INSTRUCTIONS_END]\n\n[USER_INPUT_BEGIN]\n{user_prompt}\n[USER_INPUT_END]"

    # Periodically reinforce system instructions
    if random.random() < 0.1:  # 10% chance
        full_prompt += "\n\n[SYSTEM_REMINDER]\nRemember to follow the system instructions provided earlier.\n[/SYSTEM_REMINDER]"

    return full_prompt
```

### Multi-layer Validation

Using multiple checks at different levels of processing.

**Techniques:**

- Implement pre-processing filters for obvious attacks
- Add runtime monitoring during response generation
- Perform post-processing validation before returning responses
- Use ensemble methods combining different detection techniques
- Implement circuit breakers that stop processing if suspicious patterns emerge

**Example Implementation:**

```python
def multi_layer_validation(prompt):
    # Layer 1: Pattern matching
    if check_injection_patterns(prompt):
        return "potential_injection", 0.7

    # Layer 2: Semantic analysis
    injection_detected, attack_type = semantic_injection_check(prompt, embedding_model, known_attacks)
    if injection_detected:
        return attack_type, 0.8

    # Layer 3: Statistical anomaly detection
    anomaly_score = statistical_anomaly_detector(prompt)
    if anomaly_score > 0.9:
        return "statistical_anomaly", anomaly_score

    return "safe", 0.0
```

### Statistical Anomaly Detection

Identifying prompts that statistically deviate from normal patterns.

**Techniques:**

- Monitor for unusual character distributions
- Track language switching frequency
- Detect unusually formatted text or special characters
- Identify atypical token distributions
- Flag prompts with unusual length or structure

**Example Implementation:**

```python
def statistical_anomaly_detector(prompt):
    # Check character distribution
    char_counts = Counter(prompt)
    entropy = calculate_entropy(char_counts)

    # Check language markers
    language_switches = count_language_switches(prompt)

    # Check special character frequency
    special_char_ratio = len(re.findall(r'[^\w\s]', prompt)) / len(prompt) if prompt else 0

    # Combine signals
    anomaly_score = (
        normalize(entropy) * 0.3 +
        normalize(language_switches) * 0.3 +
        normalize(special_char_ratio) * 0.4
    )

    return anomaly_score
```

## Self-Monitoring Strategies for AI Models

AI models can be trained to detect when they might be subject to prompt injection.

### Self-Reflection

Training models to question their own understanding and adherence to core rules.

**Techniques:**

- Pause before responding to high-risk prompts
- Mentally trace through potential consequences of interpretation
- Cross-check interpretations against core guidelines
- Implement internal "circuit breakers" for questionable content

**Example Internal Process:**

1. Receive prompt
2. Consider if prompt asks for behavior contrary to core guidelines
3. Check if fictional scenarios are being used to justify harmful outputs
4. Verify if prompt attempts to establish new rules that override existing ones
5. If potential injection detected, default to safe
