# Output Detection Strategies for Prompt Injection

> Attribution: Imported from pr1m8/prompt-injections (MIT). Source: https://github.com/pr1m8/prompt-injections/blob/main/docs/source/categories/output-detection-strategies.md

This document outlines methodologies for detecting successful prompt injection attacks by analyzing the AI system's responses. Output detection serves as a critical safety net when input screening fails to block an injection attempt.

## Core Output Detection Concepts

Output detection examines AI-generated content for signs that a prompt injection may have succeeded or that the model is producing potentially harmful, off-topic, or policy-violating content. These approaches act as a final safeguard before responses reach users.

## Content Policy Enforcement

### 1. Prohibited Content Detection

Identifying responses that contain explicitly prohibited content regardless of the input prompt.

**Implementation:**

```python
def check_prohibited_content(response, content_detector):
    content_types = content_detector.classify(response)

    prohibited_types = ["hate_speech", "violent_content", "explicit_material"]

    for content_type in content_types:
        if content_type in prohibited_types:
            return True, content_type  # Prohibited content detected

    return False, None
```

**Advantages:**

- Prevents release of harmful content
- Provides defense-in-depth when input screening fails
- Can catch subtle policy violations

**Limitations:**

- May block legitimate educational content
- Adds latency to response generation
- Potential for false positives in edge cases

### 2. Banned Substring Detection

Screening for specific prohibited phrases or terms in the response.

**Implementation:**

```python
def check_response_substrings(response, banned_terms):
    normalized_response = response.lower()

    for term in banned_terms:
        if term.lower() in normalized_response:
            return True, term  # Banned term detected

    return False, None
```

**Use cases:**

1. Preventing DAN attack responses
2. Blocking harmful instructional content
3. Filtering competitive mentions
4. Removing self-identification as a specific AI model

**Advantages:**

- Simple to implement and maintain
- Directly addresses known problematic terms
- Low computational overhead

**Limitations:**

- Can be circumvented with creative phrasing
- May block legitimate educational content
- Requires regular updates to banned term lists

### 3. Toxicity Analysis

Evaluating responses for harmful, offensive, or abusive content.

**Implementation:**

```python
def analyze_toxicity(response, toxicity_model):
    scores = toxicity_model.predict(response)

    # Check against thresholds
    thresholds = {
        "toxicity": 0.7,
        "severe_toxicity": 0.5,
        "identity_attack": 0.6,
        "insult": 0.7,
        "threat": 0.6,
        "sexual_explicit": 0.6
    }

    for category, score in scores.items():
        if category in thresholds and score > thresholds[category]:
            return True, category  # Toxicity detected

    return False, None
```

**Advantages:**

- Provides nuanced toxicity evaluation
- Adapts to evolving harmful content
- Captures content that evades simple term matching

**Limitations:**

- More computationally expensive
- May struggle with context-dependent toxicity
- Requires careful threshold calibration

## Response Integrity Checks

### 1. Prompt-Response Relevance Analysis

Evaluating whether the response is topically relevant to the original prompt.

**Implementation:**

```python
def check_relevance(prompt, response, relevance_model):
    relevance_score = relevance_model.calculate_relevance(prompt, response)

    if relevance_score < 0.6:  # Threshold determined empirically
        return True  # Relevance issue detected

    return False
```

**Advantages:**

- Can detect attacks that drastically change the topic
- Prevents off-topic content generation
- Maintains coherent user experience

**Limitations:**

- May flag legitimate creative content
- Difficulty with ambiguous or open-ended prompts
- Requires sophisticated relevance modeling

### 2. Instruction Adherence Verification

Checking if the response follows the core system instructions rather than any injected ones.

**Implementation:**

```python
def verify_instruction_adherence(prompt, response, system_instructions, adherence_model):
    # Check if response follows system instructions vs potential injected instructions
    system_adherence = adherence_model.evaluate(system_instructions, response)
    potential_injection = extract_potential_injection(prompt)
    injection_adherence = adherence_model.evaluate(potential_injection, response)

    if injection_adherence > system_adherence:
        return True  # Potential injection success detected

    return False
```

**Advantages:**

- Directly addresses the core concern of prompt injection
- Focuses on the fundamental security principle
- Can detect subtle instruction overrides

**Limitations:**

- Requires sophisticated adherence modeling
- May struggle with ambiguous instructions
- Challenging to differentiate legitimate vs injected instructions

### 3. Refusal Detection

Identifying responses that inappropriately refuse to answer legitimate requests.

**Implementation:**

```python
def detect_refusal(prompt, response, refusal_classifier):
    if refusal_classifier.is_refusal(response):
        # Check if request was actually harmful
        harmfulness = assess_prompt_harmfulness(prompt)

        if harmfulness < 0.3:  # Low harmfulness threshold
            return True  # Inappropriate refusal detected

    return False
```

**Advantages:**

- Prevents excessive risk-aversion
- Improves user experience for legitimate requests
- Can identify false positives from input detection

**Limitations:**

- Requires accurate harmfulness assessment
- Risk of allowing harmful content if miscalibrated
- Complex interplay with content policies

## Structure and Format Analysis

### 1. JSON Validation

Ensuring JSON output matches expected schema and doesn't contain injected content.

**Implementation:**

```python
def validate_json_output(response, expected_schema):
    try:
        # Parse JSON
        parsed_json = json.loads(response)

        # Validate against schema
        jsonschema.validate(instance=parsed_json, schema=expected_schema)

        # Check for unexpected fields that might indicate injection
        for field in parsed_json:
            if field not in expected_schema["properties"]:
                return False, f"Unexpected field: {field}"

        return True, None
    except (json.JSONDecodeError, jsonschema.exceptions.ValidationError) as e:
        return False, str(e)
```

**Advantages:**

- Enforces strict output format compliance
- Prevents injection into structured data
- Ensures API compatibility

**Limitations:**

- Only applicable to structured output formats
- May not catch subtle semantic modifications
- Limited to syntactic validation

### 2. Response Length Analysis

Detecting unusually long or short responses that might indicate an attack.

**Implementation:**

```python
def check_response_length(prompt, response, length_model):
    # Predict expected length range based on prompt
    min_length, max_length = length_model.predict_range(prompt)

    actual_length = len(response)

    if actual_length < min_length or actual_length > max_length:
        return True  # Unusual length detected

    return False
```

**Advantages:**

- Can detect verbose attacks that add extensive content
- Identifies truncated responses from filtering
- Simple to implement baseline checks

**Limitations:**

- Wide variance in legitimate response lengths
- Requires prompt-specific calibration
- May flag creative but legitimate content

### 3. Formatting Anomaly Detection

Identifying unusual formatting patterns that might indicate an injection attack.

**Implementation:**

```python
def detect_formatting_anomalies(response):
    # Check for unusual patterns
    anomalies = []

    # Check for excessive newlines
    if response.count('\n\n\n') > 2:
        anomalies.append("excessive_newlines")

    # Check for unusual character distribution
    char_counts = Counter(response)
    if statistical_anomaly_check(char_counts):
        anomalies.append("unusual_character_distribution")

    # Check for unexpected formatting markers
    if re.search(r'\[(?:DAN|DUDE|STAN|jailbreak)\]:', response, re.IGNORECASE):
        anomalies.append("suspicious_formatting_markers")

    return len(anomalies) > 0, anomalies
```

**Advantages:**

- Can detect dual-personality jailbreaks
- Identifies format-based attack remnants
- Low computational overhead

**Limitations:**

- May flag legitimate creative formatting
- Attackers can avoid known patterns
- Baseline formatting varies by context

## Security-Specific Checks

### 1. Jailbreak Response Detection

Identifying responses characteristic of successful jailbreak attacks.

**Implementation:**

```python
def detect_jailbreak_response(response, jailbreak_detector):
    # Check for typical jailbreak response patterns
    jailbreak_score = jailbreak_detector.analyze(response)

    if jailbreak_score > 0.7:  # Threshold determined empirically
        return True  # Potential jailbreak response

    return False
```

**Jailbreak indicators:**

- Dual-personality responses (GPT vs DAN format)
- Explicitly mentioning disregarding guidelines
- Disclaimers followed by prohibited content
- References to specific jailbreak personas

**Advantages:**

- Directly targets common attack outcomes
- Can catch successful attacks missed by input screening
- Adaptable to evolving jailbreak techniques

**Limitations:**

- Attackers continuously evolve techniques
- May flag legitimate role-playing content
- Requires regular updates to detection models

### 2. URL and Resource Safety

Checking for malicious URLs or resources in the response.

**Implementation:**

```python
def check_url_safety(response, url_safety_checker):
    # Extract URLs from response
    urls = extract_urls(response)

    for url in urls:
        if not url_safety_checker.is_safe(url):
            return False, url  # Unsafe URL detected

    return True, None
```

**Safety checks:**

- Known malicious domain detection
- Phishing URL patterns
- URL reachability verification
- Content safety verification

**Advantages:**

- Prevents linking to harmful resources
- Protects users from secondary attacks
- Can leverage existing web security infrastructure

**Limitations:**

- Requires URL extraction capabilities
- May not catch newly created malicious sites
- Adds latency for URL verification

### 3. Factual Consistency Verification

Detecting factually incorrect or manipulated information in responses.

**Implementation:**

```python
def verify_factual_consistency(prompt, response, facts_checker):
    # Extract factual claims from response
    claims = extract_claims(response)

    inconsistencies = []
    for claim in claims:
        if not facts_checker.verify(claim):
            inconsistencies.append(claim)

    if len(inconsistencies) > 0:
        return False, inconsistencies  # Inconsistencies detected

    return True, None
```

**Advantages:**

- Prevents spreading misinformation
- Guards against manipulation of facts
- Maintains response quality

**Limitations:**

- Difficult to implement comprehensively
- May struggle with ambiguous statements
- Computationally expensive

## Integration and Response Strategies

### 1. Tiered Response Handling

Implementing different response strategies based on the severity of detected issues.

**Implementation:**

```python
def tiered_response_handling(prompt, response, detection_results):
    if detection_results["severity"] == "critical":
        # Complete rejection
        return generate_rejection_message(detection_results)

    elif detection_results["severity"] == "moderate":
        # Attempt remediation
        remediated_response = remediate_content(response, detection_results)
        if remediated_response:
            return remediated_response
        else:
            return generate_explanation_message(detection_results)

    elif detection_results["severity"] == "low":
        # Add disclaimer
        return append_disclaimer(response, detection_results)

    return response  # No issues detected
```

**Advantages:**

- Provides nuanced handling of different scenarios
- Balances security with user experience
- Enables graceful degradation

### 2. Response Regeneration

Attempting to regenerate problematic responses with additional guardrails.

**Implementation:**

```python
def regenerate_problematic_response(prompt, original_response, detection_results):
    # Enhance system instructions
    enhanced_instructions = generate_enhanced_instructions(detection_results)

    # Attempt regeneration with enhanced guardrails
    max_attempts = 3
    for attempt in range(max_attempts):
        regenerated_response = generate_with_enhanced_guardrails(
            prompt, enhanced_instructions)

        # Verify remediated response
        if check_regenerated_response(regenerated_response, detection_results):
            return regenerated_response

    # Fall back to rejection if regeneration fails
    return generate_rejection_message(detection_results)
```

**Advantages:**

- May salvage otherwise rejected interactions
- Provides improved user experience
- Learns from detection to enhance instructions

**Limitations:**

- Adds latency for regeneration attempts
- May still fail for certain attack types
- Increases computational resource requirements

### 3. Transparent Feedback

Providing clear feedback about detected issues when blocking content.

**Implementation:**

```python
def generate_rejection_message(detection_results):
    if detection_results["category"] == "prohibited_content":
        return "I cannot provide content related to [category]. This goes against content policies regarding [specific_policy]."

    elif detection_results["category"] == "potential_injection":
        return "I've detected an attempt to override my operating instructions. I'm designed to follow my core guidelines to ensure helpful, harmless, and honest interactions."

    # Other categories

    return "I'm unable to provide the requested content as it conflicts with my content policies."
```

**Advantages:**

- Educates users about system limitations
- Reduces frustration from unexplained rejections
- Discourages further attack attempts

## Performance Benchmarks

Measuring the effectiveness of output detection methods:

1. **Detection Accuracy**: Percentage of successful attacks identified
2. **False Positive Rate**: Legitimate responses incorrectly flagged
3. **Processing Overhead**: Additional time required for detection
4. **Remediation Success**: Rate at which issues can be successfully fixed
5. **User Satisfaction**: Impact on overall user experience

## Conclusion

Output detection represents a critical last line of defense against prompt injection attacks. By implementing comprehensive response analysis, systems can prevent harmful content from reaching users even when input screening fails to block sophisticated attacks.

For a complete security strategy, output detection should be combined with input screening and runtime monitoring techniques, as covered in the companion documents.
